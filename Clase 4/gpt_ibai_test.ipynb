{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de CPU ó GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usar GPU\n"
     ]
    }
   ],
   "source": [
    "# Establecer el valor de la semilla en todos lados para hacerlo reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Usar GPU\")\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    # Indicar a PyTorch que ejecute este modelo en la GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    batch_size = 1\n",
    "\n",
    "else:\n",
    "    print(\"Usar CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos el Modelo de HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Diego\\anaconda3\\envs\\gpu\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizador de GPT.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\")\n",
    "control_code = \"ibai\"\n",
    "special_tokens_dict = {\n",
    "         \"additional_special_tokens\": ['f\"<|{control_code}|>\"'],\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "unk_tok_emb = model.transformer.wte.weight.data[tokenizer.unk_token_id, :]\n",
    "for i in range(num_added_toks):\n",
    "        model.transformer.wte.weight.data[-(i+1), :] = unk_tok_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos el Dataset “Ibai_textos.txt”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "  def __init__(self, control_code, tokenizer, archivo_texto = 'ibai_textos.txt', max_length=768):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "    print('loading text...')\n",
    "    sentences = open(archivo_texto, 'r', encoding=\"utf-8\").read().lower().split('\\n')\n",
    "    print('qty:',len(sentences))\n",
    "    for row in tqdm(sentences):\n",
    "      encodings_dict = tokenizer('<|startoftext|>'+ f\"<|{control_code}|>\" + row + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text...\n",
      "qty: 10292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10292/10292 [00:03<00:00, 2846.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,189 muestras de entrenamiento\n",
      "  103 muestras de validación\n"
     ]
    }
   ],
   "source": [
    "dataset = GPT2Dataset(control_code, tokenizer, archivo_texto=\"ibai_textos.txt\", max_length=768)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "train_size = int(0.99 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} muestras de entrenamiento'.format(train_size))\n",
    "print('{:>5,} muestras de validación'.format(val_size))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # Las muestras de entrenamiento.\n",
    "            sampler = RandomSampler(train_dataset), # Selecciona lotes de forma aleatoria.\n",
    "            batch_size = batch_size # Entrena con este tamaño de lote.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos haciendo el Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "sample_every = 500\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar el modelo GPT-2 con nuestro Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   500  of  10,189. Loss: 0.4329083561897278.   Elapsed: 0:01:02.\n",
      "  Batch 1,000  of  10,189. Loss: 0.25056442618370056.   Elapsed: 0:02:03.\n",
      "  Batch 1,500  of  10,189. Loss: 0.4402444660663605.   Elapsed: 0:03:05.\n",
      "  Batch 2,000  of  10,189. Loss: 0.1298457384109497.   Elapsed: 0:04:08.\n",
      "  Batch 2,500  of  10,189. Loss: 0.26451703906059265.   Elapsed: 0:05:10.\n",
      "  Batch 3,000  of  10,189. Loss: 0.46704205870628357.   Elapsed: 0:06:12.\n",
      "  Batch 3,500  of  10,189. Loss: 0.17926152050495148.   Elapsed: 0:07:15.\n",
      "  Batch 4,000  of  10,189. Loss: 0.19153498113155365.   Elapsed: 0:08:17.\n",
      "  Batch 4,500  of  10,189. Loss: 0.279425710439682.   Elapsed: 0:09:19.\n",
      "  Batch 5,000  of  10,189. Loss: 0.3570004105567932.   Elapsed: 0:10:21.\n",
      "  Batch 5,500  of  10,189. Loss: 0.14839531481266022.   Elapsed: 0:11:23.\n",
      "  Batch 6,000  of  10,189. Loss: 0.15792593359947205.   Elapsed: 0:12:26.\n",
      "  Batch 6,500  of  10,189. Loss: 0.11067229509353638.   Elapsed: 0:13:28.\n",
      "  Batch 7,000  of  10,189. Loss: 0.19821593165397644.   Elapsed: 0:14:30.\n",
      "  Batch 7,500  of  10,189. Loss: 0.23188818991184235.   Elapsed: 0:15:32.\n",
      "  Batch 8,000  of  10,189. Loss: 0.48981261253356934.   Elapsed: 0:16:34.\n",
      "  Batch 8,500  of  10,189. Loss: 0.13703210651874542.   Elapsed: 0:17:36.\n",
      "  Batch 9,000  of  10,189. Loss: 0.1665278524160385.   Elapsed: 0:18:38.\n",
      "  Batch 9,500  of  10,189. Loss: 0.11335351318120956.   Elapsed: 0:19:41.\n",
      "  Batch 10,000  of  10,189. Loss: 0.23571671545505524.   Elapsed: 0:20:43.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:21:06\n",
      "Training complete!\n",
      "Total training took 0:21:07 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_t0 = time.time()\n",
    "model = model.to(device)\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(  b_input_ids, labels=b_labels, \n",
    "                          attention_mask = b_masks, token_type_ids=None )\n",
    "        loss = outputs[0]\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    t0 = time.time()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar el modelo Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando modelo en ./model_gpt_ibai/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_gpt_ibai/tokenizer_config.json',\n",
       " './model_gpt_ibai/special_tokens_map.json',\n",
       " './model_gpt_ibai/vocab.json',\n",
       " './model_gpt_ibai/merges.txt',\n",
       " './model_gpt_ibai/added_tokens.json',\n",
       " './model_gpt_ibai/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardando buenas prácticas: si usas los nombres predeterminados para el modelo, puedes recargarlo usando from_pretrained()\n",
    "output_dir = './model_gpt_ibai/'\n",
    "\n",
    "# Crear el directorio de salida si es necesario\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Guardando modelo en %s\" % output_dir)\n",
    "\n",
    "# Guarda un modelo entrenado, su configuración y el tokenizador usando `save_pretrained()`.\n",
    "# Luego pueden recargarse usando `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Manejar entrenamiento distribuido/paralelo\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Texto con el nuevo Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <|ibai|>para mi la vida es una película espectacular.  que no me lo esperaba.  que no me lo esperaba.  si fuera un clásico, ¿ dónde me gustaría ir a ir ? creo que iría de la manera más cómoda. \n",
      "\n",
      "\n",
      "1: <|ibai|>para mi la vida es lo que más me molesta.  tengo una vida en la que tengo un montón de cosas que no me pueden valer.  tengo unas ideas, un estilo de vida que estoy haciendo totalmente diferente a lo que estoy haciendo ahora mismo.  llevo ya dos años en la empresa y estoy totalmente de acuerdo con mi jefa, mi mejor amigo y mi mejor aliado. \n",
      "\n",
      "\n",
      "2: <|ibai|>para mi la vida es una puta mierda.  y los perros también.  los perros son asesinos, la verdad.  ¿ qué pasa ? de hecho, yo tengo una perra que se llama giant. \n",
      "\n",
      "\n",
      "3: <|ibai|>para mi la vida es lo que he hecho.  y la vida también es como mi hija, que es lo que intento con la vida.  mi hija a mí me gusta mucho que tenga su espacio, me guste su espacio y, a veces, no le doy ni tiempo ni espacio a mi familia y no me entero.  y cuando voy a otro lugar para verla, también me molesta. \n",
      "\n",
      "\n",
      "4: <|ibai|>para mi la vida es corta.  mi vida es corta.  mi vida es corta.  no. \n",
      "\n",
      "\n",
      "5: <|ibai|>para mi la vida es increíble.  la vida es increíble.  yo, por ejemplo, cuando he viajado a madrid, he viajado mucho a madrid.  no sé qué, pero, pero, pero, porque como que, o sea, en verano  no, en verano, en vacaciones no, en vacaciones no. \n",
      "\n",
      "\n",
      "6: <|ibai|>para mi la vida es tan complicada que no entiendo cómo, cuando todo el mundo está haciendo las cosas por su cuenta, por la cuenta de una de las cadenas más importantes de televisión mundial, hace falta un poco de criterio, para llegar a los que se la tiran encima.  un poco de criterio de de qué hacer si es para el interés general, pero no de dar un trato privilegiado a los que se lo tiran encima.  es un tema que la cadena que más ha llegado a la final es la cadena de twitch, que hace cosas que no hace la cadena d\n",
      "\n",
      "\n",
      "7: <|ibai|>para mi la vida es un momento feliz.  o sea, yo lo recomiendo a todo el mundo porque no sé si te va a gustar, a todo el mundo, te va a gustar y es un entretenimiento y tal.  porque te lo puede dar todo el mundo, te puede gustar hasta un punto que te puede gustar.  te puede gustar y lo puede dar todo el mundo. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"<|startoftext|>\" + \"<|\"+control_code+\"|>\" + \"para mi la vida es\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "#print(generated)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=50, \n",
    "                                max_length = 150,\n",
    "                                top_p=0.95, \n",
    "                                num_return_sequences=8\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el modelo...\n",
      "¡Bienvenido al BootcampGPT del Bootcamp de Ciencia de Datos & IA.!\n",
      "Escribe 'salir' para terminar la conversación.\n",
      "\n",
      "Tú: conoces al kun aguero?\n",
      "ChatGPT: e es muy fan de kun, me dicen que me ha hecho mucho reír con él.  me ha hecho reír de todo, me dice que me está escuchando.  sí, pero es que me hace reír de todo.  ¿ qué es lo que me hace reír ? no sé si le da risa o qué es lo que me hace reír.\n",
      "Tú: hola conoces al kun aguero?\n",
      "ChatGPT: a que hace cosas de las que se puede sentir orgulloso.  no solo a nivel de periodismo, sino en otros muchos ámbitos.  ha sido un profesional que ha pasado por muchas personas, que han cambiado su vida, su forma de ser, de ver el fútbol.  es un periodista que ha cambiado la vida de muchos, de muchos.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Ruta del modelo entrenado\n",
    "model_path = \"./model_gpt_ibai\"\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "print(\"Cargando el modelo...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()  # Establecer en modo evaluación\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Parámetros de generación\n",
    "def generate_response(prompt, max_length=150, top_k=30, top_p=0.85):\n",
    "    \"\"\"\n",
    "    Genera una respuesta a partir del prompt dado.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Retornar solo la parte generada (sin el prompt inicial)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Validar respuestas generadas\n",
    "def is_valid_response(response):\n",
    "    \"\"\"\n",
    "    Filtra respuestas incoherentes o irrelevantes.\n",
    "    \"\"\"\n",
    "    invalid_keywords = [\"puta madre\", \"cago\", \"irrelevante\", \"sin sentido\"]\n",
    "    for word in invalid_keywords:\n",
    "        if word in response.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Chatbox interactivo\n",
    "print(\"¡Bienvenido al BootcampGPT del Bootcamp de Ciencia de Datos & IA.!\")\n",
    "print(\"Escribe 'salir' para terminar la conversación.\\n\")\n",
    "\n",
    "# Contexto inicial\n",
    "context = \"<|startoftext|><|ibai|> Esto es una conversación con Ibai. Él responde preguntas y ayuda en lo que puede.\\n\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Tú: \")\n",
    "    if user_input.lower() in [\"salir\", \"exit\", \"quit\"]:\n",
    "        print(\"ChatGPT: ¡Adiós!\")\n",
    "        break\n",
    "    \n",
    "    # Crear el prompt con contexto\n",
    "    prompt = context + f\"Tú: {user_input}\\n<|ibai|> \"\n",
    "    \n",
    "    # Generar respuesta\n",
    "    response = generate_response(prompt)\n",
    "    \n",
    "    # Validar respuesta; regenerar si es inválida\n",
    "    while not is_valid_response(response):\n",
    "        response = generate_response(prompt)\n",
    "    \n",
    "    # Mostrar respuesta\n",
    "    print(f\"ChatGPT: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dace661734e3bdd5ce646b3c7d78fb8723b198e4b1bab33f3b3f594e15ac9154"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
